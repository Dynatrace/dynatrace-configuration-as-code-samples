{"version":"6","defaultTimeframe":{"from":"now()-2h","to":"now()"},"defaultSegments":[],"sections":[{"id":"06b61cc2-9236-4668-be1d-84c4886e9937","type":"markdown","markdown":"# üöÄ AzureDevOps Releases & Builds Integration\nThe purpose is to help you integrate your AzureDevops Releases and Builds with Dynatrace in order to visualize statistics, execution logs and alerts."},{"id":"986a6763-e91a-4c41-bd29-e18503a6cfed","type":"markdown","markdown":"### üìã Step 1 - Pre-requisites\n\n- Access to your Dynatrace Tenant and permission to create tokens\n- Your tenant ID. Which can be found on your environment URL. https://<YOUR_TENANT_ID>.live.dynatrace.com/\n- A token with **Ingest Logs v2** scope\n- A token with **Write Settings** scope"},{"id":"7a1f1d61-e07d-43ba-9d29-06f2b906bb8e","type":"markdown","markdown":"### ü™ù Step 2 - Creating Webhooks in AzureDevops\n\n - First, we need to create two Service Hooks Subscriptions on Azure. One for **Builds** Completed and one for **Release Deployment** Completed. \n\t - You can create it on `https://{orgName}/{project_name}/_settings/serviceHooks`\n\t - [AzureDevOps Webhooks](https://learn.microsoft.com/en-us/azure/devops/service-hooks/services/webhooks?toc=%2Fazure%2Fdevops%2Fmarketplace-extensibility%2Ftoc.json&view=azure-devops)\n - During the configuration, do not apply any filter\n - In the settings page of the Subscription, you need to fill the following fields accordantly\n\t - URL: `https://<YOUR_TENANT_ID>.live.dynatrace.com/api/v2/logs/ingest`\n\t - HTTP Headers:\n\t\t - `\"Authorization: Api-token <YOUR_LOG_INGEST_TOKEN>\"`\n\t\t - **The text above must be written exactly like that. Copy and paste and just change the token**\n\t - Change \"Messages to send\" and \"Detailed Messages to send\" to **Text**\nThat's all you need in AzureDevops!"},{"id":"a08bf42c-a438-4fb5-8bc2-579f6564bae8","type":"markdown","markdown":"### ü™£ Step 3 - Configuring new Dynatrace Log Bucket\n\n#### Follow the below steps to create a new logs bucket:\n* Open the **Storage Management** app in your tenant\n  - https://<YOUR_TENANT_ID>.apps.dynatrace.com/ui/apps/dynatrace.storage.management/\n* Create a new bucket using the + sign on the top right corner\n  - Setup the bucket name = **azure_devops_logs**\n  - Set the retention time as desired\n  - Set bucket type as logs"},{"id":"b431d856-edce-49a2-a6b9-42fda0f7073c","type":"markdown","markdown":"### üßë‚Äçüíª Step 4 - Configuring OpenPipeline - Log Processing Rules\n\n#### \n* Go to OpenPipeline > Logs > Pipelines\n* Create a new pipeline, and name it \"AzureDevOps\"\n* Go to Dynamic Routing and create the following new rule\n  * `matchesPhrase(eventType,\"ms.vss-release.deployment-completed-event\") OR matchesPhrase(eventType,\"build.complete\")`\n* Set the Pipeline dropdown to \"AzureDevOps\"\n* Return back to your pipelines, and open AzureDevOps\n* Under Storage section, add a new processor > Bucket assignment and set the storage to **azure_devops_logs** bucket.\n* Go to Processing section:\n  * Create a new rule for Rename Fields - \"Rename Build Fields\"\n    * Add Name & Value pairs\n      * buildNumber:`resource.buildNumber`\n      * result:`resource.result`\n  * Create a new rule for Rename Fields - \"Rename Release Fields\"\n    * Add Name & Value pairs\n      * stageName:`resource.stageName`\n      * projectName:`resource.project.name`\n      * releaseName:`resource.deployment.release.name`\n      * releaseStatus:`resource.environment.status`\n  * Add sample data to each of the new rules from logs to verify the rule is working as exepected"},{"id":"3e64dc55-fcec-499e-9db0-70a69060018a","type":"markdown","markdown":"### üñ•Ô∏è Step 5 - Dashboard Template (on Logs)\n* Go to [AzureDevOps Git Repository](https://github.com/rohanshah-sre/AzureDevOps/tree/main), and download `AzureDevOps Dashboard (on Logs).json` file.\n* Within Dynatrace, open the new Dashboards app, and press **Upload** button at the top left corner.\n* Upload the json file to start visualizing your Azure DevOps data."},{"id":"5e390a52-447a-495f-bb15-da1861ac3240","type":"markdown","markdown":"## üßÆ Advanced Mode\n\n> Taking this a step further, we can convert log data to events for each release and build, and discard the logs after to reduce the amount of unnecessary logs retained. "},{"id":"95ef8860-b68d-4c6b-991f-745924e01488","type":"markdown","markdown":"### üíº Step 6 - Configuring OpenPipeline - Business Events Extraction\n\n* Go to OpenPipeline > Logs > Pipelines > AzureDevOps\n* Go to Data Extraction section:\n  * Create a new rule Business Event Processor - \"Build result\"\n    * Matching condition: `matchesPhrase(eventType,\"build.complete\")`\n    * Event type: `eventType`\n    * Event provider (change to Static String): `AzureDevOps`\n    * Select *Fields to extract*\n      * `result`\n      * `buildNumber`\n      * `resource.reason`\n  * Create a new rule Business Event Processor - \"Release result\"\n    * Matching condition: `matchesPhrase(eventType,\"ms.vss-release.deployment-completed-event\")`\n    * Event type: `eventType`\n    * Event provider (change to Static String): `AzureDevOps`\n    * Select *Fields to extract*\n      * `stageName`\n      * `projectName`\n      * `releaseName`\n      * `releaseStatus`\n      * `resource.deployment.startedOn`\n      * `resource.deployment.completedOn`\n      \n##### ‚ö†Ô∏è (Optional) - Davis Events Extraction\n* Go to Data Extraction section:\n  * Create a new rule *Davis* Event Processor - \"Build complete failed\"\n    * Matching condition: `matchesPhrase(eventType,\"build.complete\") AND result==\"failed\"` \n    * Event Name: `Build Complete Failed`\n    * Event Description: `Unable to generate build {buildNumber}`\n    * *event.type* can be changed from CUSTOM_ALERT to increase the severity level. (listed below)\n  * Create a new rule *Davis* Event Processor - \"Release Rejected\"\n    * Matching condition: `matchesPhrase(eventType,\"ms.vss-release.deployment-completed-event\") and releaseStatus == \"rejected\"` \n    * Event Name: `Release deployment rejected`\n    * Event Description: `Unable to deploy release {releaseName}`\n    * *event.type* can be changed from CUSTOM_ALERT to increase the severity level. (listed below)\n  * Available event types:\n    * `AVAILABILITY_EVENT`\n    * `CUSTOM_ALERT`\n    * `CUSTOM_ANNOTATION`\n    * `CUSTOM_CONFIGURATION`\n    * `CUSTOM_DEPLOYMENT`\n    * `CUSTOM_INFO`\n    * `ERROR_EVENT`\n    * `MARKED_FOR_TERMINATION`\n    * `PERFORMANCE_EVENT`\n    * `RESOURCE_CONTENTION_EVENT`\n\n##### ‚õìÔ∏è‚Äçüí• (Optional) - Discarding logs storage\nOnce the logs have been converted to Business and Davis events to extract the information required, we can go ahead and disable the storage assignment rule in the pipeline. \n* Go to OpenPipeline > Logs > Pipelines > AzureDevOps\n* Under Storage > Bucket assignment Rule > Change the following rule:\n  * Matching condition: `false`"},{"id":"58e8df9e-c675-4d0c-8282-6e268d4087ca","type":"markdown","markdown":"### üñ•Ô∏è Step 7 - Dashboard Template (on Events)\n\n* Go to [AzureDevOps Git Repository](https://github.com/rohanshah-sre/AzureDevOps/tree/main)Ôªø, and download `AzureDevOps Dashboard (on BizEvents).json` file.\n* Within Dynatrace, open the new Dashboards app, and press Upload button at the top left corner.\n* Upload the json file to start visualizing your Azure DevOps data."},{"id":"576418f6-ed60-4e09-ba9d-0ce7524f1b13","type":"markdown","markdown":"### üì¶ Step 8 - Segments\nSegments are typically modeled to allow filtering monitored entities, logs, metrics, events, and other types of data. \n\nWe will use it to filter *AzureDevOps* data\n* Go to Segments App > Create a new segment using + sign on the top right corner\n* Rename it to `AzureDevOps`\n* Add Business events data type\n  * Input `event.provider = AzureDevOps`\n* Add Logs data type\n  * Input `dt.system.bucket = azure_devops_logs`\n* Press Save"},{"id":"b49db776-ccaa-4e7f-9cad-c6b4828a2615","type":"markdown","markdown":"## üéâ You're all set, enjoy Dynatrace integration\n\n - Keep in mind, you might need to request a Log Content Length (MaxContentLength_Bytes) increase depending on how many steps your Release Events have.\n - The integration consumes DDUs for Log Ingest and Log Metrics and will depend on how many build/release events you have. For comparison purposes, a customer with 400 events was consuming around 1 DDU per week"}]}